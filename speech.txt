NVIDIA CUDA SPEECH


1. INIZIO

2. Ho scelto questo argomento perché nella mia carriera accademica non ho mai avuto modo di studiare le gpu e di come si
programmano in particolare. E soprattutto perché si parla spesso del loro ruolo fondamentale in questo momento storico
di boom per l’intelligenza artificiale?

3. Che cos’è CUDA? CUDA è l’acronimo di Compute Unified Device Architecture, è un estensione del linguaggio C/C++ e
permette agli sviluppatori di eseguire programmi sulle GPU. Oggi le GPU non si limitano ad essere utilizzate solo per la
grafica, ma sono usate ad esempio in applicazioni scientifiche e nel deep learning. CUDA è quindi quel layer intermedio
che permette agli sviluppatori di eseguire applicazioni sulle GPU per algoritmi tipicamente destinati alle CPU, aprendo
le porte al GPGPU (General-purpose computing on graphics processing)

4. Prima di entrare nel dettaglio di CUDA è necessario però avere un’idea di come le GPU funzionano a livello
architetturale. Le GPU sono essenzialmente delle macchine SIMD, composte da molti core che eseguono istruzioni
in-order, cioè nell’ordine in cui sono specificate nel programma

5. Ma cos’è SIMD? SIMD, acronimo di Single Instruction Multiple data, è una tipologia di parallel processing delle
istruzioni che consiste nell’eseguire la stessa operazione su più dati distinti nello stesso momento sfruttando il
cosiddetto DLP (Data Level Parallelism). Durante il corso abbiamo studiato un’altro tipo di approccio al parallelismo
applicato alle CPU, chiamato ILP (instruction level parallelism), ad esempio attraverso lo scoreboarding o Tomasulo. Il
requisito fondamentale del DLP è che i dati non siano dipendenti tra loro e quindi si possa evitare di implementare una
logica per il controllo delle dipendenze che ad esempio sono presenti nelle cpu, risparmiando così area di silicio che
possono essere sfruttati in altro modo. Si possono distinguere due tipi di processori che implementano l’approccio SIMD:
- array processors
- vector processors

6. La differenza principale tra questi due processori risiede nella modalità di esecuzione delle istruzioni. Gli array
processors replicano le unità funzionali ed eseguono la stessa istruzione simultaneamente su ciascuna di esse. I vector
processors, invece, funzionano più come una pipeline, eseguendo le istruzioni in fasi diverse sullo stesso processore.
In pratica, questa distinzione è teorica, poiché le GPU moderne combinano caratteristiche di entrambi i tipi di
processori.

7. In questo esempio abbiamo un’architettura in grado di gestire 32 elementi simultanei a 8 linee, che combina sia un
array che un vector processor

8. Tra i vantaggi di SIMD troviamo l’efficienza energetica, poiché richiede meno cicli di fetch e decode, meno
istruzioni di branch e riduce il tempo di inattività. Inoltre, consente di eseguire più istruzioni nello stesso ciclo di
clock, aumentando il throughput. Un altro vantaggio è che non necessita di hardware per il controllo delle dipendenze
sui dati, utilizzando meno area di silicio.

D'altra parte, SIMD presenta alcuni svantaggi. Gli algoritmi devono essere ripensati e riprogrammati in funzione della
specifica architettura, rendendoli difficili da programmare. Inoltre, SIMD funziona solo se i dati sono privi di
dipendenze e risulta molto inefficiente se i dati non sono regolari in memoria, ovvero se i pattern di accesso non sono
uniformi.

9. Venendo alle GPU, il concetto di SIMD viene astratto attraverso un’altro modello chiamato SIMT (single instruction
multiple thread) dove le istruzioni sono eseguite su dei THREAD, che rappresentano la più piccola unità logica che può
essere eseguita su una GPU, non sono da confondere con core fisici. Ogni thread è una sequenza di istruzioni che opera
su dati diversi che la GPU può eseguire in maniera parallela sui propri core fisici. La differenza con SIMD è che in
questo modello lo sviluppatore non deve gestire manualmente la vettorizzazione dei dati, ma può continuare a scrivere
codice in modo sequenziale senza preoccuparsi di quello che succede a livello hardware. La GPU a livello hardware
raggruppa i thread in blocchi da 32 WARPs che eseguono la stessa istruzione nello stesso momento, condividendo di fatto
lo stesso program counter.

In sostanza il modello SIMT delle GPU offre una maggiore scalabilità e flessibilità rispetto al modello SIMD, in quanto
implementa il cosiddetto FGMT (Finegrained multi-threading) facendo si che la gpu possa gestire un gran numero di thread
in parallelo e permettendo percorsi di esecuzione divergenti. Mentre SIMD richiede che tutte le unità eseguano la stessa
istruzione contemporaneamente, SIMT consente a ogni thread di avere il proprio contesto di esecuzione.

10. Ad esempio, il concetto di warp è visualizzato qui come uno stack di WARP che vengono schedulati ed eseguiti in una
pipeline SIMD. Ogni WARP è composto da 32 thread che eseguono la stessa istruzione nello stesso momento e condividono lo
stesso program counter. In caso di branch la gpu gestisce il PC in sottoinsiemi di thread, fino a che i path non si
ricongiungono

11. Forse da togliere

12. La stessa immagine che abbiamo visto in precedenza per SIMD, può essere adattata per il modello SIMT dove al posto
dell’issuing delle singole istruzioni abbiamo l’issuing di WARPs

13. Qui possiamo vedere la distinzione di fatto tra un array processor & un vector processor e come l'elaborazione dei
dati avviene in parallelo su più elementi.

14. Ma dove vengono effettivamente eseguiti questi thread? Prendendo come esempio l’architettura Fermi, introdotta nel
2008 da Nvidia i thread vengono eseguiti su piccoli CORE, chiamati CUDA core, che risiedono all'interno di strutture
chiamate Streaming Multiprocessor. Ogni Streaming Multiprocessor è composto da 32 CUDA cores, 8 Special Function Units,
4 Load/Store Units e 16 Double Precision Units. La funzione dello Streaming Multiprocessor è quella di schedulare
l'esecuzione dei WARPs, gestire la memoria condivisa e la cache L1. Può essere messo a confronto, di fatto, con una
processore classico: si occupa dell'instruction fetch, il decode, l'execute, il memory access e il writeback, i branch,
le load e le store e così via. Si possono anche notare un Register File da 32KB e 64KB di L1 cache.

15. Mettendo a confronto uno schema, molto semplificato, di una CPU & di un GPU possiamo notare queste essenziali
differenze: le CPU hanno pochi, ma molto potenti, core in grado di eseguire le istruzioni in a out-of-order fashion
mentre le GPU hanno molti più core, meno potenti e semplici che eseguono le istruzioni in-order. Le CPU sono pensate per
eseguire programmi ed algoritmi general purpose, mentre le gpu sono limitate in questo aspetto, sono più adatte ad
algoritmi che risultano essere facilmente parallelizzabili dove i dati non sono dipendenti tra di loro. CPU e GPU
collaborano insieme durante l’esecuzione di un programma dove le parti più adatte ad essere eseguite sulla gpu vengono
‘scaricate’ su di essa per migliorare e velocizzare l’esecuzione dell’applicazione.
16. Quindi, avendo capito a grandi linee come funziona una gpu, possiamo finalmente arrivare a parlare di come possiamo
eseguire dei programmi su di essa. Per prima cosa è bene chiarire la terminologia, in particolare in CUDA le CPU sono
chiamate HOST, mentre le GPU device. Notare che un sistema può essere dotato di N device e di M CPU.Come avviene l’avvio
di un’esecuzione su di un device? In sostanza si ha che la CPU per prima cosa copia i dati necessari all’esecuzione
nella memoria della GPU, lancia l’esecuzione sulla GPU che una volta terminata copia il risultato verso la memoria della
cpu.
17. In CUDA, è necessario distinguere e capire 3 concetti principali: - il primo, che abbiamo già visto sono i thread,
che sono la più piccola unità logica che la gpu gestisce ed esegue all’interno dei cuda core- poi abbiamo i blocks, che
non sono altro che un raggruppamento di thread a livello software da non confondere con i warp che rappresentano
l’implementazione hardware. A livello hardware questi possono essere visti come un’astrazione degli Streaming
Multiprocessor, i quali schedulano l’esecuzione dei propri thread i quali hanno accesso alla stessa memoria condivisa-
ed infine le grids che rappresentano una collezione di blocks, distribuiti su molteplici Streaming Multiprocessors
18. Immagine che descrive quanto detto sopra
19. Qui abbiamo un po’ di funzioni di base che CUDA espone al programmatore,- troviamo ad esempio la definizione
fondamentale di kernel, che non rappresenta altro che una funzione eseguita sulla GPU- la malloc per copiare allocare
memoria sulla gpu- cudaMemCpy per la copia dei dati dalla cpu alla gpu e viceversa- come si lancia un kernel su una GPU-
come si libera memoria dalla GPU- e come è possibile sincronizzare i thread in esecuzione
20. Come fa il programmatore ad accedere ad un determinato thread in esecuzione? Come vengono allocati in memoria i
thread? Ci sono essenzialmente 3 modi diversi che il programmatore può scegliere: 1D (vettore), 2D (matrice) e 3D. A
seconda della disposizione dei thread in memoria ogni thread può essere acceduto attraverso l’uso di queste variabili
disponibili che rappresentano la posizione del thread nel block, del block nella grid, quanti thread sono disponibili in
ogni block in ogni direzione e quanti blocks sono disponibili in ogni griglia in ogni direzione
21. La rappresentazione a una dimensione è la più semplice, dove i dati sono disposti in row-major layout e ogni thread
viene acceduto tramite in questo modo
22. La rappresentazione a Matrice invece è un pochino più complessa x
23. Un concetto importante per le performance dei programmi CUDA è capire organizzare i dati in memoria in modo tale che
la GPU possa eseguire nel modo più efficiente possibile. Questo concetto viene chiamato coalescing, sopra abbiamo un
esempio di accesso not coalesced dove i dati sono organizzati in row major. L’accesso non è ottimizzato perchè i thread,
nella stessa esecuzione non accedono a dati contigui perciò non è possibile per la GPU prelevare, in uno scenario
ipotetico, i dati necessari in un unico accesso alla memoria introducendo quindi latenza nell’operazione.Nell’immagine
sotto invece i dati sono organizzati in column-major e la GPU è in grado di leggere i dati in un’unica transazione verso
la memoria. Questo accesso è altamente ottimizzato e viene detto coalesced memory access
24. Il concetto di latency hiding invece è una proprietà derivata dal modello SIMT, in cui lo scheduler della GPU in
ogni streaming processor può effettuare lo switch di contesto dei warp che per un determinato motivo sono in attesa e
rimpiazzarli invece con un warp libero di eseguire portando così la gpu ad avere una migliore occupazione, definita come
il rapporto tra i warp attivi e il numero massimo di warp eseguibili sullo streaming multiprocessor
25. Come vengono gestiti i branch nel codice? Abbiamo visto che l’architettura SIMD riduce la necessità delle istruzioni
condizionali, senza però ridurle del tutto. In caso di branch, ogni thread all’interno di un warp può prendere ‘path’
diversi creando la cosiddetta intrawrap-divergence riducendo così l’efficienza generale della gpu. La risoluzione a
questo problema è lasciata al programmatore, infatti è compito suo definire in modo appropriato i branch in modo che non
si verifichi l’intrawarp-divergence
26. In questo esempio, il primo blocco di codice a sinistra mostra una gestione dei branch non ottimizzata mentre quello
a destra permette 32 thread, ovvero un intero warp, di eseguire una strada piuttosto che l’altra, andando di fatto a
mantenere piena efficienza a livello di warp
27. La shared memory è una memoria di piccole dimensioni, ma molto veloce, che abbiamo visto nell’immagine di prima ed è
utilizzata dai thread appartenenti ad uno stesso block per accedere e condividere i dati in modo efficiente, è
chiaramente molto più veloce della global memory ma ha una capienza minore. È di fatto la cache L1. È divisa in più
banks per garantire l’accesso simultaneo ai dati da thread differenti. È gestibile dal programmatore tramite la
direttiva __shared__. Dato che la gestione della memoria non è automatizzata possono verificarsi dei conflitti che ne
vanno a ridurre l’efficacia
28. Un modo per ‘evitare’ il tradizionale passaggio di dati che abbiamo introdotto all’inizio tra host e device sarebbe
quello di sfruttare la unified memory che è stata resa disponibile all’incirca verso CUDA 6 (2013). Questa memoria non è
altro che uno spazio condiviso sia dall’host che dal device che possono così condividere i dati senza dovere
necessariamente sincronizzarsi a vicenda. A livello hardware questa memoria può essere sia completamente destinata nella
ram della cpu, della gpu oppure può essere divisa tra le due a seconda della configurazione desiderata. Questo tipo di
accesso alla memoria va a semplificare il codice e la gestione generale dei dati in quanto è l’hardware a gestire
automaticamente l’allocazione, la migrazione e la deallocazione dei dati. Per questo motivo può introdurre overhead
quando si verificano numerosi page fault (ovvero l’host o il device non trovano i dati in memoria locale ed è necessario
una migrazione di questi)
