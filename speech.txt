NVIDIA CUDA SPEECH


1. INIZIO

2. Ho scelto questo argomento perché nella mia carriera accademica non ho mai avuto modo di studiare le gpu e di come si
programmano in particolare. E soprattutto perché si parla spesso del loro ruolo fondamentale in questo momento storico
di boom per l’intelligenza artificiale?

3. Che cos’è CUDA? CUDA è l’acronimo di Compute Unified Device Architecture, è un estensione del linguaggio C/C++ e
permette agli sviluppatori di eseguire programmi sulle GPU. Oggi le GPU non si limitano ad essere utilizzate solo per la
grafica, ma sono usate ad esempio in applicazioni scientifiche e nel deep learning. CUDA è quindi quel layer intermedio
che permette agli sviluppatori di eseguire applicazioni sulle GPU per algoritmi tipicamente destinati alle CPU, aprendo
le porte al GPGPU (General-purpose computing on graphics processing)

4. Prima di entrare nel dettaglio di CUDA è necessario però avere un’idea di come le GPU funzionano a livello
architetturale. Le GPU sono essenzialmente delle macchine SIMD, composte da molti core che eseguono istruzioni
in-order, cioè nell’ordine in cui sono specificate nel programma

5. Ma cos’è SIMD? SIMD, acronimo di Single Instruction Multiple data, è una tipologia di parallel processing delle
istruzioni che consiste nell’eseguire la stessa operazione su più dati distinti nello stesso momento sfruttando il
cosiddetto DLP (Data Level Parallelism). Durante il corso abbiamo studiato un’altro tipo di approccio al parallelismo
applicato alle CPU, chiamato ILP (instruction level parallelism), ad esempio attraverso lo scoreboarding o Tomasulo. Il
requisito fondamentale del DLP è che i dati non siano dipendenti tra loro e quindi si possa evitare di implementare una
logica per il controllo delle dipendenze che ad esempio sono presenti nelle cpu, risparmiando così area di silicio che
possono essere sfruttati in altro modo. Si possono distinguere due tipi di processori che implementano l’approccio SIMD:
- array processors
- vector processors

6. La differenza principale tra questi due processori risiede nella modalità di esecuzione delle istruzioni. Gli array
processors replicano le unità funzionali ed eseguono la stessa istruzione simultaneamente su ciascuna di esse. I vector
processors, invece, funzionano più come una pipeline, eseguendo le istruzioni in fasi diverse sullo stesso processore.
In pratica, questa distinzione è teorica, poiché le GPU moderne combinano caratteristiche di entrambi i tipi di
processori.

7. In questo esempio abbiamo un’architettura in grado di gestire 32 elementi simultanei a 8 linee, che combina sia un
array che un vector processor

8. Tra i vantaggi di SIMD troviamo l’efficienza energetica, poiché richiede meno cicli di fetch e decode, meno
istruzioni di branch e riduce il tempo di inattività. Inoltre, consente di eseguire più istruzioni nello stesso ciclo di
clock, aumentando il throughput. Un altro vantaggio è che non necessita di hardware per il controllo delle dipendenze
sui dati, utilizzando meno area di silicio.

D'altra parte, SIMD presenta alcuni svantaggi. Gli algoritmi devono essere ripensati e riprogrammati in funzione della
specifica architettura, rendendoli difficili da programmare. Inoltre, SIMD funziona solo se i dati sono privi di
dipendenze e risulta molto inefficiente se i dati non sono regolari in memoria, ovvero se i pattern di accesso non sono
uniformi.

9. Venendo alle GPU, il concetto di SIMD viene astratto attraverso un’altro modello chiamato SIMT (single instruction
multiple thread) dove le istruzioni sono eseguite su dei THREAD, che rappresentano la più piccola unità logica che può
essere eseguita su una GPU, non sono da confondere con core fisici. Ogni thread è una sequenza di istruzioni che opera
su dati diversi che la GPU può eseguire in maniera parallela sui propri core fisici. La differenza con SIMD è che in
questo modello lo sviluppatore non deve gestire manualmente la vettorizzazione dei dati, ma può continuare a scrivere
codice in modo sequenziale senza preoccuparsi di quello che succede a livello hardware. La GPU a livello hardware
raggruppa i thread in blocchi da 32 WARPs che eseguono la stessa istruzione nello stesso momento, condividendo di fatto
lo stesso program counter.

In sostanza il modello SIMT delle GPU offre una maggiore scalabilità e flessibilità rispetto al modello SIMD, in quanto
implementa il cosiddetto FGMT (Finegrained multi-threading) facendo si che la gpu possa gestire un gran numero di thread
in parallelo e permettendo percorsi di esecuzione divergenti. Mentre SIMD richiede che tutte le unità eseguano la stessa
istruzione contemporaneamente, SIMT consente a ogni thread di avere il proprio contesto di esecuzione.

10. Ad esempio, il concetto di warp è visualizzato qui come uno stack di WARP che vengono schedulati ed eseguiti in una
pipeline SIMD. Ogni WARP è composto da 32 thread che eseguono la stessa istruzione nello stesso momento e condividono lo
stesso program counter. In caso di branch la gpu gestisce il PC in sottoinsiemi di thread, fino a che i path non si
ricongiungono

11. Forse da togliere

12. La stessa immagine che abbiamo visto in precedenza per SIMD, può essere adattata per il modello SIMT dove al posto
dell’issuing delle singole istruzioni abbiamo l’issuing di WARPs

13. Qui possiamo vedere la distinzione di fatto tra un array processor & un vector processor e come l'elaborazione dei
dati avviene in parallelo su più elementi.

14. Ma dove vengono effettivamente eseguiti questi thread? Prendendo come esempio l’architettura Fermi, introdotta nel
2008 da Nvidia i thread vengono eseguiti su piccoli CORE, chiamati CUDA core, che risiedono all'interno di strutture
chiamate Streaming Multiprocessor. Ogni Streaming Multiprocessor è composto da 32 CUDA cores, 8 Special Function Units,
4 Load/Store Units e 16 Double Precision Units. La funzione dello Streaming Multiprocessor è quella di schedulare
l'esecuzione dei WARPs, gestire la memoria condivisa e la cache L1. Può essere messo a confronto, di fatto, con una
processore classico: si occupa dell'instruction fetch, il decode, l'execute, il memory access e il writeback, i branch,
le load e le store e così via.

15. Mettendo a confronto uno schema, molto semplificato, di una CPU & di un GPU possiamo notare queste essenziali
differenze: le CPU hanno pochi, ma molto potenti, core in grado di eseguire le istruzioni in a out-of-order fashion
mentre le GPU hanno molti più core, meno potenti e semplici che eseguono le istruzioni in-order. Le CPU sono pensate per
eseguire programmi ed algoritmi general purpose, mentre le gpu sono limitate in questo aspetto, sono più adatte ad
algoritmi che risultano essere facilmente parallelizzabili dove i dati non sono dipendenti tra di loro. CPU e GPU
collaborano insieme durante l’esecuzione di un programma dove le parti più adatte ad essere eseguite sulla gpu vengono
‘scaricate’ su di essa per migliorare e velocizzare l’esecuzione dell’applicazione.

16. Quindi, avendo capito a grandi linee come funziona una gpu, possiamo finalmente arrivare a parlare di come possiamo
eseguire dei programmi su di essa. Per prima cosa è bene chiarire la terminologia, in particolare in CUDA le CPU sono
chiamate HOST, mentre le GPU device. Notare che un sistema può essere dotato di N device e di M CPU. Sapendo quindi che,
un'applicazione è comunque una collaborazione tra questi dispositivi, come avviene l’avvio di un’esecuzione su di un
device? In sostanza si ha che la CPU per prima cosa copia i dati necessari all’esecuzione nella memoria della GPU,
lancia l’esecuzione sulla GPU (lancia un kernel) che una volta terminata copia il risultato verso la memoria della cpu.

17. In CUDA, è necessario distinguere e capire 3 concetti principali: 
- il primo, che abbiamo già visto sono i thread, che sono la più piccola unità logica che la gpu gestisce ed esegue 
all’interno dei cuda core
- poi abbiamo i blocks, che non sono altro che un raggruppamento di thread a livello software da non confondere con i
warp che rappresentano l’implementazione hardware. A livello hardware questi possono essere visti come un’astrazione
degli Streaming Multiprocessor, i quali schedulano l’esecuzione dei propri thread i quali hanno accesso alla stessa
memoria condivisa
- ed infine le grids che rappresentano una collezione di blocks, distribuiti su molteplici Streaming Multiprocessors

18. Immagine che descrive quanto detto sopra

19. Qui ci sono alcune delle API che espone CUDA al programmatore:
- troviamo ad esempio la definizione fondamentale di kernel, che non rappresenta altro che una funzione eseguita sulla GPU
- la cudaMalloc per allocare memoria sulla gpu
- cudaMemCpy per la copia dei dati dalla cpu alla gpu e viceversa
- come si lancia un kernel su una GPU
- come si libera memoria dalla GPU
- e come è possibile sincronizzare i thread in esecuzione

20. Come fa il programmatore ad accedere ad un determinato thread in esecuzione? Come vengono allocati in memoria i
thread? Ci sono essenzialmente 3 modi diversi che il programmatore può scegliere: 1D (vettore), 2D (matrice) e 3D. A
seconda della disposizione dei thread in memoria ogni thread può essere acceduto attraverso l’uso di queste variabili
disponibili che rappresentano la posizione del thread nel block, del block nella grid, quanti thread sono disponibili in
ogni block in ogni direzione e quanti blocks sono disponibili in ogni griglia in ogni direzione

21. La rappresentazione a una dimensione è la più semplice, dove i dati sono disposti in row-major layout e ogni thread
viene acceduto tramite in questo modo

22. La rappresentazione a Matrice invece è un pochino più complessa x

23. Un aspetto fondamentale per ottimizzare le prestazioni in CUDA è l'organizzazione ottimale dei dati in memoria, che
permette alla GPU di operare con massima efficienza. Questo concetto è noto come coalescing. Nell'esempio citato,
vediamo un strided access con dati organizzati in row-major; qui i thread accedono a dati non contigui, creando uno
stride. Questo impedisce alla GPU di recuperare tutti i dati necessari in un unico accesso alla memoria, aumentando così
la latenza. Al contrario, con una disposizione column-major, i thread accedono a dati consecutivi, consentendo alla GPU
di realizzare operazioni di memoria ottimizzate in un'unica transazione. Questo tipo di accesso, detto coalesced memory
access, è estremamente efficiente e riduce significativamente il numero di accessi alla memoria.

24. Il concetto di latency hiding invece è una proprietà derivata dal modello SIMT, in cui lo scheduler della GPU in
ogni streaming processor può effettuare lo switch di contesto dei warp che per un determinato motivo sono in attesa e
rimpiazzarli invece con un warp libero di eseguire portando così la gpu ad avere una migliore occupazione, definita come
il rapporto tra i warp attivi e il numero massimo di warp eseguibili sullo streaming multiprocessor

25. In CUDA, l'architettura SIMD implica una gestione particolare dei branch nel codice. Sebbene questa architettura
riduca la necessità di istruzioni condizionali, non le elimina completamente. Quando si verifica un branch, ogni thread
all'interno di un warp può seguire percorsi diversi, creando la cosiddetta 'intrawarp divergence'. Questa divergenza
riduce l'efficienza generale della GPU poiché tutti i thread di un warp devono attendere fino a quando non si
riconvergono. Per minimizzare questo problema, spetta al programmatore strutturare i branch in modo che la divergenza
intrawarp sia ridotta, ottimizzando così l'esecuzione parallela.

26. In questo esempio, il primo blocco di codice a sinistra mostra una gestione dei branch non ottimizzata mentre quello
a destra permette 32 thread, ovvero un intero warp, di eseguire una strada piuttosto che l’altra, andando di fatto a
mantenere piena efficienza a livello di warp

27. La shared memory è un tipo di memoria di piccole dimensioni ma ad alta velocità utilizzata dai thread di uno stesso
block per condividere dati in modo efficiente. Rispetto alla global memory, offre tempi di accesso notevolmente
inferiori, sebbene abbia una capacità limitata. Nonostante sia simile alla cache L1, non è automatizzata e richiede una
gestione esplicita tramite la direttiva __shared__. La shared memory è organizzata in banks, che permettono
accessi simultanei da parte di diversi thread. Tuttavia, questa organizzazione può portare a conflitti di bank, i quali
riducono l'efficienza di accesso e, di conseguenza, l'efficacia complessiva della memoria condivisa.

28. Un modo per ‘evitare’ il tradizionale passaggio di dati che abbiamo introdotto all’inizio tra host e device sarebbe
quello di sfruttare la unified memory che è stata resa disponibile all’incirca verso CUDA 6 (2013). Questa memoria non è
altro che uno spazio condiviso sia dall’host che dal device che possono così condividere i dati senza dovere
necessariamente sincronizzarsi a vicenda. A livello hardware questa memoria può essere sia completamente destinata nella
ram della cpu, della gpu oppure può essere divisa tra le due a seconda della configurazione desiderata. Questo tipo di
accesso alla memoria va a semplificare il codice e la gestione generale dei dati in quanto è l’hardware a gestire
automaticamente l’allocazione, la migrazione e la deallocazione dei dati. Per questo motivo può introdurre overhead
quando si verificano numerosi page fault (ovvero l’host o il device non trovano i dati in memoria locale ed è necessario
una migrazione di questi)

29. Possiamo finalmente arrivare a parlare di un esempio pratico di come si può scrivere un programma in CUDA. In questo
caso abbiamo un semplice programma che effettua una SAXPY, ovvero a Single-Precision A*X Plus Y, operazione molto comune
nel calcolo scientifico. La keyword __global__ indica che la funzione è eseguita sulla GPU, vengono passati: la
dimensione massima dei vettori n, lo scalare a e i puntatori ai vettori x e y. Dato che stiamo parlando di vettori,
l'allocazione in memoria e l'accesso ai thread avviene in 1 dimensione.

30. Il codice dell'host, ovvero la CPU, consiste nella maggior parte nella preparazione dei dati e nella chiamata del
kernel. Possiamo vedere che l'operazione è eseguita su un campione di 500_000_000 (500 milioni) di float a singola
precisione, che vengono allocati in ram (occupando circa 3.75 GB di ram) sull'host, poi copiati nella memoria della GPU,
vengono elaborati dal kernel e infine copiati nuovamente nella ram dell'host. Un punto fondamentale da notare è la parte
di esecuzione del kernel, dove si specifica la dimensione del blocco e quanti saranno necessari per processare tutti gli
elementi dei vettori. In questo caso ho definito una blockSize di 256 thread per blocco.
Notare che non viene usata la unified memory in questo esempio.

31. Per compilare questo programma, abbiamo bisogno del compilatore CUDA, chiamato NVCC. Il compilatore prende il codice
sorgente, divide il codice in parti eseguibili sulla CPU e sulla GPU, compila il codice CUDA che viene tradotto in PTX
che non rappresenta altro che un assembly di alto livello per la GPU, e infine il PTX viene compilato in codice macchina
per la GPU, chiamato SASS, che è specifico per l'architettura della GPU usata. Il codice compilato per la CPU e per la GPU viene poi linkato insieme per creare l'eseguibile finale.

32. Eseguendo questo codice sul mio computer fisso con queste specifiche, abbiamo un tempo di esecuzione totale di circa 
15 millisecondi

33. Possiamo vedere gli step citati prima di allocazione ed esecuzione del kernel nel Task Manager del computer

34. E possiamo infine notare la differenza che lo stesso codice, scritto per una CPU senza sfruttare il multithreading,
sia estremamente più lento rispetto alla versione scritta per la GPU

35. In conclusione ho imparato che, CUDA è un'api per poter abilitare la programmazione parallela su GPU, il modo di
programmare una GPU non è tanto diverso da quello di una CPU, ma richiede una conoscenza più approfondita dell'hardware
sottostante e soprattutto di come i dati vengono gestiti in memoria.
